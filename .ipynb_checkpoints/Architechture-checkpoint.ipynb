{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture For Exercise 2\n",
    "##### w205 by Nikki Haas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Architecture.pdf: complete documentation (max 4 pages) of your Twitter application, including directory and file structure, application idea, description of the architecture, file dependencies, any necessary information to run the application, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Twitter Application\n",
    "\n",
    "My Twitter application will bring back a stream of original tweets (no retweets) from Twitter, parse them into single words, put them into a Postgres Database, and keep count of their frequencies.  To use this Twitter application, use an AWS instance that has Linux, Python 2.7, Postgres, and Apache Storm installed.  Apache Storm is an opensource platform that facilitates the storing and manipulation of streaming data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Directories and File Structure\n",
    "\n",
    "##### topologies\n",
    "\n",
    "Apache Storm applications rely on a specific file structure.  Topologies are where the hierarchical information for the storm app lives.  A topology tells storm which files are bolt files, which are spout files, and which order to use them in. In this example, my topologies folder contains a file called `tweetwordcount.clj` written in the coljure language.  It tells Storm that the spout file is called `tweets` and to create three spouts for the application.  It also informs spout that we'll want to use two bolt files named `parse` and `wordcount1`, and that we'll need 3 parse bolts and 2 wordcount1 bolts. \n",
    "\n",
    "##### src: \n",
    "\n",
    "Apache Storm depends on the bolts and spout being contained in their own subdirectories in an src directory. \n",
    "\n",
    "##### scr: spouts\n",
    "Spouts collect or generate streaming data. In this case, I have one spout written in python called `tweets.py`.  To use this spout, you will have to edit it to include your own Twitter credentials.  If you do not have Twitter developer credentials, you will have to sign up for a twitter app [here](https://apps.twitter.com/).  This file uses the tweepy python library to connect to Twitter and stream data to the bolts.  It is commanded to run until the a keyboard interrupt is entered by the user.\n",
    "\n",
    "##### src: bolts\n",
    "\n",
    "Bolts manipulate, transform, or store data collected from the spout. This application uses two bolts:  one to parse and format the tweets called `parse.py` and one to shunt the words into a Postgres DB and update the word count called `wordcount1.py`.\n",
    "\n",
    "`parse.py`:  This is a rather simple parser that implements the regex library `re` for a streamlined approach.  It first checks the tweets for retweets, replies, mentions, urls and hashtags and disregards those.  Then, it splits the tweets into individual words on the space, strips out non-alphanumeric character.  The words from each tweet are put into a list of words and sent along to the next bolt.\n",
    "\n",
    "`wordcount1.py`: The word counter file makes use of the `psycopg2` python library.  Psycopg2 is a connector between Python and Postgres.  It takes the lists of words from processes them further by removing non alphanumeric characters within the words themselves, lowercasing them, and checks the database for the words existence.  If the word does not exist in the database, it is added with a count of 1.  If the word does exist in the database, its count is increased by 1.  This program treats the word as a primary key in the postgres table, which is why it was important to normalize it as much as possible.  The `psycopg2` library works by opening a connection to Postgres, committing and edit, then closing the connection when the job is done.  The Postgres sever must be running before this job is run, however, so take care to run the readme.sh file before starting your project.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Application Suggetions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running the Application\n",
    "\n",
    "Please follow the instructions in the readme file, which can also be run as a shell script on your properly formatted AWS to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
